{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount Google Drive**"
      ],
      "metadata": {
        "id": "SkxUnycX7UFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-sEuhLSC7SKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Neccesary Packages**"
      ],
      "metadata": {
        "id": "HVgLaKvh7cVG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-0rdN7acn_V"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets scikit-learn seaborn\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import CLIPProcessor, CLIPTokenizer, CLIPModel, get_scheduler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "import glob\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuration for Subtask D**"
      ],
      "metadata": {
        "id": "z3rKKy_Y_Pqx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz6GlzJZIQuo"
      },
      "outputs": [],
      "source": [
        "class CFG_D:\n",
        "    \"\"\"\n",
        "    Configuration Class for Subtask D: Humor Detection.\n",
        "    \"\"\"\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/case-2025\"\n",
        "\n",
        "    # File & Directory Paths\n",
        "    train_dir = os.path.join(DRIVE_PATH, \"SubTaskD/Train\")\n",
        "    train_text_path = os.path.join(DRIVE_PATH, \"SubTaskD/Train/STask_D_train.csv\")\n",
        "    val_image_dir = os.path.join(DRIVE_PATH, \"SubTaskD/Eval/STask_D_val_img\")\n",
        "    val_labels_path = os.path.join(DRIVE_PATH, \"SubTaskD/Eval/STask-D(index,label)val.csv\")\n",
        "    val_text_path = os.path.join(DRIVE_PATH, \"SubTaskD/Eval/STask-D(index,text)val.csv\")\n",
        "    test_image_dir = os.path.join(DRIVE_PATH, \"SubTaskD/Test/STask_D_test_img\")\n",
        "    test_csv_path = os.path.join(DRIVE_PATH, \"SubTaskD/Test/STask-D(index,text)test.csv\")\n",
        "    output_dir = os.path.join(DRIVE_PATH, \"output_subtask_d_hyper\")\n",
        "\n",
        "    #Model & Training Parameters\n",
        "    model_name = 'openai/clip-vit-large-patch14'\n",
        "    image_size = 224\n",
        "    max_token_len = 77\n",
        "    learning_rate_base = 1e-6\n",
        "    learning_rate_head = 1e-5\n",
        "    batch_size = 16\n",
        "    epochs = 8\n",
        "    num_workers = 2\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "os.makedirs(CFG_D.output_dir, exist_ok=True)\n",
        "print(f\" Subtask D Configuration defined. Output will be saved to: {CFG_D.output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading**"
      ],
      "metadata": {
        "id": "hLAQuEfh_tT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    #Training Data Loading\n",
        "    train_image_paths = glob.glob(os.path.join(CFG_D.train_dir, '**/*.png'), recursive=True)\n",
        "    train_data = []\n",
        "    class_folders = ['Humor', 'No Humor']\n",
        "    for path in train_image_paths:\n",
        "        label = os.path.basename(os.path.dirname(path))\n",
        "        if label in class_folders:\n",
        "            train_data.append({'index': os.path.basename(path), 'label_text': label})\n",
        "\n",
        "    ground_truth_labels_df = pd.DataFrame(train_data)\n",
        "    text_data_df = pd.read_csv(CFG_D.train_text_path, usecols=['index', 'text'])\n",
        "    train_df_d = pd.merge(ground_truth_labels_df, text_data_df, on=\"index\")\n",
        "\n",
        "    # Validation Data Loading\n",
        "    val_labels_df = pd.read_csv(CFG_D.val_labels_path)\n",
        "    val_text_df = pd.read_csv(CFG_D.val_text_path)\n",
        "    val_df_d = pd.merge(val_text_df, val_labels_df, on=\"index\")\n",
        "\n",
        "    print(\"Data loaded successfully.\")\n",
        "    print(f\"Training samples:   {len(train_df_d)}\")\n",
        "    print(f\"Validation samples: {len(val_df_d)}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n ERROR: A data file was not found. Please double-check your paths in CFG_D.\")\n",
        "    print(f\" Details: {e}\")\n",
        "    sys.exit()"
      ],
      "metadata": {
        "id": "38YofkkG_nvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization: Label Distribution"
      ],
      "metadata": {
        "id": "HOuQ-nCX_yss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.figure(figsize=(8, 5))\n",
        "class_order = ['No Humor', 'Humor']\n",
        "sns.countplot(x='label_text', data=train_df_d, palette='rocket', order=class_order)\n",
        "plt.title('Subtask D: Training Set Label Distribution')\n",
        "plt.xlabel('Humor Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NrMPa39A_zKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PREPARATION**"
      ],
      "metadata": {
        "id": "J5lPtLQK_5i-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzLzdiKYg-og"
      },
      "outputs": [],
      "source": [
        "#Define the Official Mapping\n",
        "official_target_map_d = {\n",
        "    'No Humor': 0,\n",
        "    'Humor': 1\n",
        "}\n",
        "\n",
        "# Apply the Mapping\n",
        "train_df_d['label_encoded'] = train_df_d['label_text'].map(official_target_map_d)\n",
        "val_df_d.rename(columns={'label': 'label_encoded'}, inplace=True)\n",
        "val_df_d['label_encoded'] = val_df_d['label_encoded'].astype(int)\n",
        "\n",
        "\n",
        "# Calculate Class Weights for the Loss Function\n",
        "print(\"\\n Calculating class weights to handle data imbalance.\")\n",
        "class_counts = train_df_d['label_encoded'].value_counts().sort_index()\n",
        "print(f\"   - Class counts: {class_counts.to_dict()}\")\n",
        "weights = len(train_df_d) / (len(class_counts) * class_counts)\n",
        "CFG_D.class_weights = torch.tensor(weights.values, dtype=torch.float).to(CFG_D.device)\n",
        "print(f\"Calculated weights for loss function: {CFG_D.class_weights}\")\n",
        "\n",
        "# Store the number of classes in config.\n",
        "CFG_D.num_classes = len(official_target_map_d)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BUILDING THE DATA PIPELINE**"
      ],
      "metadata": {
        "id": "iuCNkNQqAM0l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_LQrmzbhJwM"
      },
      "outputs": [],
      "source": [
        "#Load the Official CLIP Processor\n",
        "processor = CLIPProcessor.from_pretrained(CFG_D.model_name)\n",
        "\n",
        "#Define the Dataset Class\n",
        "class HumorDataset(Dataset):\n",
        "    def __init__(self, df, processor, image_dir, image_size, max_token_len, is_test=False):\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.image_dir = image_dir\n",
        "        self.image_size = image_size\n",
        "        self.max_token_len = max_token_len\n",
        "        self.is_test = is_test\n",
        "        self.image_path_map = {os.path.basename(p): p for p in glob.glob(os.path.join(image_dir, '**/*.*'), recursive=True)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = str(row.get('text', ''))\n",
        "\n",
        "        tokenized = self.processor(text=text, truncation=True, max_length=self.max_token_len, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        image_name = row['index']\n",
        "        image_path = self.image_path_map.get(image_name)\n",
        "        if image_path:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        else:\n",
        "            image = Image.new('RGB', (self.image_size, self.image_size), 'black')\n",
        "        processed_image = self.processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "        item = {\n",
        "            'input_ids': tokenized['input_ids'].squeeze(0),\n",
        "            'attention_mask': tokenized['attention_mask'].squeeze(0),\n",
        "            'pixel_values': processed_image['pixel_values'].squeeze(0)\n",
        "        }\n",
        "        if not self.is_test:\n",
        "            item['label'] = torch.tensor(row['label_encoded'], dtype=torch.long)\n",
        "        return item"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loaders**"
      ],
      "metadata": {
        "id": "-MnHC5lxAYhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_d = pd.read_csv(CFG_D.test_csv_path)\n",
        "\n",
        "train_dataset_d = HumorDataset(train_df_d, processor, CFG_D.train_dir, CFG_D.image_size, CFG_D.max_token_len)\n",
        "val_dataset_d = HumorDataset(val_df_d, processor, CFG_D.val_image_dir, CFG_D.image_size, CFG_D.max_token_len)\n",
        "test_dataset_d = HumorDataset(test_df_d, processor, CFG_D.test_image_dir, CFG_D.image_size, CFG_D.max_token_len, is_test=True)\n",
        "\n",
        "train_loader_d = DataLoader(train_dataset_d, batch_size=CFG_D.batch_size, shuffle=True, num_workers=CFG_D.num_workers)\n",
        "val_loader_d = DataLoader(val_dataset_d, batch_size=CFG_D.batch_size, shuffle=False, num_workers=CFG_D.num_workers)\n",
        "test_loader_d = DataLoader(test_dataset_d, batch_size=CFG_D.batch_size, shuffle=False, num_workers=CFG_D.num_workers)\n",
        "\n",
        "print(f\"DataLoaders created.\")\n",
        "print(f\"Training batches:   {len(train_loader_d)}\")\n",
        "print(f\"Validation batches: {len(val_loader_d)}\")\n",
        "print(f\"Test batches:       {len(test_loader_d)}\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "wtB_oKxeAWoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Main Model Architecture**"
      ],
      "metadata": {
        "id": "KM0qWodqAimz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVc_fKhLhmyW"
      },
      "outputs": [],
      "source": [
        "class HumorClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(model_name)\n",
        "        projection_dim = self.clip.projection_dim\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2 * projection_dim, projection_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(projection_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        outputs = self.clip(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            pixel_values=pixel_values\n",
        "        )\n",
        "        image_features = outputs.image_embeds\n",
        "        text_features = outputs.text_embeds\n",
        "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instantiate the Model for Subtask D**"
      ],
      "metadata": {
        "id": "QAsu4KeQAvg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_d = HumorClassifier(\n",
        "    model_name=CFG_D.model_name,\n",
        "    num_classes=CFG_D.num_classes\n",
        ").to(CFG_D.device)\n",
        "print(f\"Model instantiated with {CFG_D.num_classes} output classes.\")\n"
      ],
      "metadata": {
        "id": "vAjHIn8gAt-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the Training Engine**"
      ],
      "metadata": {
        "id": "AXKdu6dsA3_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(weight=CFG_D.class_weights)\n",
        "print(f\"Using weighted CrossEntropyLoss with weights: {CFG_D.class_weights.cpu().numpy()}\")\n",
        "\n",
        "optimizer = AdamW([\n",
        "    {'params': model_d.clip.parameters(), 'lr': CFG_D.learning_rate_base},\n",
        "    {'params': model_d.classifier.parameters(), 'lr': CFG_D.learning_rate_head}\n",
        "], weight_decay=1e-2)\n",
        "\n",
        "num_training_steps = CFG_D.epochs * len(train_loader_d)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=int(0.1 * num_training_steps),\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "print(\"Optimizer and LR Scheduler are ready.\")"
      ],
      "metadata": {
        "id": "P4yAfYeLBExC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper Functions**"
      ],
      "metadata": {
        "id": "mnEHtwLxBFPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n",
        "    for batch in progress_bar:\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids, attention_mask, pixel_values)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(loader, desc=\"Validating\", leave=False)\n",
        "        for batch in progress_bar:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask, pixel_values)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    return avg_loss, accuracy, precision, recall, f1\n",
        "\n",
        "print(\"Training and validation helper functions are ready.\")"
      ],
      "metadata": {
        "id": "YOqcO4C_BKhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Training Loop**"
      ],
      "metadata": {
        "id": "nV5zKDslBM4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_d = {\n",
        "    'train_loss': [], 'val_loss': [], 'val_accuracy': [],\n",
        "    'val_precision': [], 'val_recall': [], 'val_f1': []\n",
        "}\n",
        "best_val_f1 = 0.0\n",
        "model_path_d = os.path.join(CFG_D.output_dir, 'best_model_subtask_d.pth')\n",
        "\n",
        "print(\"\\n Starting Subtask D Fine-Tuning: \")\n",
        "for epoch in range(CFG_D.epochs):\n",
        "    print(f\"\\n===== Epoch {epoch + 1}/{CFG_D.epochs} =====\")\n",
        "    train_loss = train_one_epoch(model_d, train_loader_d, optimizer, criterion, lr_scheduler, CFG_D.device)\n",
        "    val_loss, val_acc, val_prec, val_rec, val_f1 = validate_one_epoch(model_d, val_loader_d, criterion, CFG_D.device)\n",
        "\n",
        "    history_d['train_loss'].append(train_loss); history_d['val_loss'].append(val_loss)\n",
        "    history_d['val_accuracy'].append(val_acc); history_d['val_precision'].append(val_prec)\n",
        "    history_d['val_recall'].append(val_rec); history_d['val_f1'].append(val_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} Summary:\")\n",
        "    print(f\"  - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  - Accuracy: {val_acc:.4f}, F1-Score (Macro): {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        print(f\"New best F1-score! Saving model to {model_path_d}\")\n",
        "        torch.save(model_d.state_dict(), model_path_d)\n",
        "    else:\n",
        "        print(\"F1-score did not improve.\")\n",
        "\n",
        "print(\"\\n Training Finished\")\n",
        "print(f\"Best validation F1-Score for Subtask D achieved: {best_val_f1:.4f}\")"
      ],
      "metadata": {
        "id": "wUu6fKUDAs1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting the Training History**"
      ],
      "metadata": {
        "id": "_xs03bweBawU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.plot(history_d['train_loss'], 'r-o', label='Train Loss')\n",
        "ax1.plot(history_d['val_loss'], 'orange', marker='o', label='Validation Loss')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend(loc='upper left')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(history_d['val_f1'], 'b-x', label='Validation F1-Score (Macro)')\n",
        "ax2.set_ylabel('F1-Score (Macro)')\n",
        "ax2.legend(loc='upper right')\n",
        "plt.title('Subtask D: Training and Validation History')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8XK3hgLzBY5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prediction on Test Set**"
      ],
      "metadata": {
        "id": "5bbraWECTQ1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Prediction Function\n",
        "def predict_subtask_d(model_path, test_loader, device):\n",
        "    model = HumorClassifier(\n",
        "        model_name=CFG_D.model_name,\n",
        "        num_classes=CFG_D.num_classes\n",
        "    ).to(device)\n",
        "\n",
        "    # Load the saved weights from the best model path saved in the folder\n",
        "    print(f\"--> Loading best model weights from: {model_path}\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(test_loader, desc=\"Predicting on Test Set\")\n",
        "        for batch in progress_bar:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask, pixel_values)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    return all_preds\n",
        "\n",
        "# Run the Prediction and Create the Submission File\n",
        "model_path = os.path.join(CFG_D.output_dir, 'best_model_subtask_d.pth')\n",
        "predictions = predict_subtask_d(model_path, test_loader_d, CFG_D.device)\n",
        "\n",
        "indices = test_df_d['index'].tolist()\n",
        "\n",
        "print(\"\\n--> Creating JSON Lines submission file...\")\n",
        "# Define the output path.\n",
        "submission_path = os.path.join(CFG_D.output_dir, 'submission.json')\n",
        "\n",
        "with open(submission_path, 'w') as f:\n",
        "    for index, prediction in zip(indices, predictions):\n",
        "        result = {\n",
        "            \"index\": index,\n",
        "            \"prediction\": int(prediction)\n",
        "        }\n",
        "        f.write(json.dumps(result) + '\\n')\n",
        "\n",
        "print(f\"\\n Submission file for Subtask D created successfully at: {submission_path}\")\n"
      ],
      "metadata": {
        "id": "IPTL-i_4WN3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}