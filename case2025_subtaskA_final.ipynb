{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount Google Drive**\n"
      ],
      "metadata": {
        "id": "M9q4Z9qlwRSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DIkVgACKuLf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets scikit-learn seaborn"
      ],
      "metadata": {
        "id": "13FP9gQCuMms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Neccesary Packages**"
      ],
      "metadata": {
        "id": "fmxccf2_wcec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvMoqHCk6AUS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import CLIPProcessor, CLIPTokenizer, CLIPModel, get_scheduler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "import glob\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function for Configuring Files and Hyperparamters**"
      ],
      "metadata": {
        "id": "CgmJ8noywilB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYUOKvbJK-UB"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/case-2025\"\n",
        "\n",
        "    train_csv_path = os.path.join(DRIVE_PATH, \"SubTaskA/Train/ocr_train_text_label.csv\")\n",
        "    train_image_dir = os.path.join(DRIVE_PATH, \"SubTaskA/Train\")\n",
        "    val_image_dir = os.path.join(DRIVE_PATH, \"SubTaskA/Eval/STask_A_val_img\")\n",
        "    val_labels_path = os.path.join(DRIVE_PATH, \"SubTaskA/Eval/(index,label)val.csv\")\n",
        "    val_text_path = os.path.join(DRIVE_PATH, \"SubTaskA/Eval/(index,text)val.csv\")\n",
        "    test_image_dir = os.path.join(DRIVE_PATH, \"SubTaskA/Test/STask_A_test_img\")\n",
        "    test_csv_path = os.path.join(DRIVE_PATH, \"SubTaskA/Test/(index,text)test.csv\")\n",
        "    output_dir = os.path.join(DRIVE_PATH, \"output_clip_v2\")\n",
        "\n",
        "\n",
        "    # Model & Preprocessing Parameters\n",
        "    model_name = 'openai/clip-vit-large-patch14'\n",
        "    max_token_len = 77 # CLIP's fixed token length\n",
        "\n",
        "    #Hyperparameters\n",
        "    learning_rate_base = 1e-6 # For the CLIP model base\n",
        "    learning_rate_head = 1e-5 # For our custom classifier head\n",
        "    batch_size = 16\n",
        "    epochs = 5\n",
        "    num_workers = 2\n",
        "\n",
        "    # Hardware & Environment\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Output directory to store our results and models\n",
        "os.makedirs(CFG.output_dir, exist_ok=True)\n",
        "print(f\"--> Output will be saved to: {CFG.output_dir}\")\n",
        "\n",
        "# Initial Data Loading & Reconnaissance\n",
        "try:\n",
        "    # Load the primary training dataframe\n",
        "    train_df = pd.read_csv(CFG.train_csv_path)\n",
        "\n",
        "    # Load and merge the two separate validation files\n",
        "    val_labels_df = pd.read_csv(CFG.val_labels_path)\n",
        "    val_text_df = pd.read_csv(CFG.val_text_path)\n",
        "    val_df = pd.merge(val_text_df, val_labels_df, on=\"index\")\n",
        "    print(\"Data loaded successfully.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\nFATAL ERROR: A data file was not found. Please double-check your DRIVE_PATH in the CFG.\")\n",
        "    print(f\"   - Details: {e}\")\n",
        "    # Stop execution if data isn't found\n",
        "    sys.exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training and Validatio DataFrame**"
      ],
      "metadata": {
        "id": "JGFRbAu8w2vM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qCBan8dl9cMd"
      },
      "outputs": [],
      "source": [
        "    train_df.info()\n",
        "    val_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning and Data Preparation**"
      ],
      "metadata": {
        "id": "BW2eS_R7xBOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD11ug0n_dtE"
      },
      "outputs": [],
      "source": [
        "# Data Cleaning: Correcting Data Types\n",
        "print(\"Cleaning data: Converting label columns to string type...\")\n",
        "train_df['label'] = train_df['label'].astype(str)\n",
        "val_df['label'] = val_df['label'].astype(str)\n",
        "print(\"Label columns converted.\")\n",
        "\n",
        "# Data Preparation: Enforcing Official Label Encoding\n",
        "print(\"\\nPreparing data: Enforcing OFFICIAL competition label map...\")\n",
        "\n",
        "# The map connecting the CSV label value to the folder name.\n",
        "label_to_folder_map = {\n",
        "    '1': 'Hate',\n",
        "    '0': 'No Hate'\n",
        "}\n",
        "print(f\"CSV-to-Folder Map: {label_to_folder_map}\")\n",
        "train_df['folder_name'] = train_df['label'].map(label_to_folder_map)\n",
        "val_df['folder_name'] = val_df['label'].map(label_to_folder_map)\n",
        "\n",
        "official_target_map = {\n",
        "    'No Hate': 0,\n",
        "    'Hate': 1\n",
        "}\n",
        "print(f\"Offical Model Target Map: {official_target_map}\")\n",
        "\n",
        "train_df['label_encoded'] = train_df['folder_name'].map(official_target_map)\n",
        "val_df['label_encoded'] = val_df['folder_name'].map(official_target_map)\n",
        "print(\"Final target 'label_encoded' column created according to official rules.\")\n",
        "\n",
        "reverse_target_map = {v: k for k, v in official_target_map.items()}\n",
        "\n",
        "CFG.num_classes = len(official_target_map)\n",
        "print(f\"Number of classes: {CFG.num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Visualization**"
      ],
      "metadata": {
        "id": "QYxkPBxbxEOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Visualization\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "sns.countplot(x='folder_name', data=train_df, ax=ax[0], palette='magma', order=['No Hate', 'Hate'])\n",
        "ax[0].set_title('Training Set Label Distribution')\n",
        "ax[0].set_xlabel('Class')\n",
        "ax[0].set_ylabel('Count')\n",
        "train_df['text_length'] = train_df['text'].str.split().str.len().fillna(0)\n",
        "sns.histplot(train_df['text_length'], bins=30, kde=True, ax=ax[1], color='indigo')\n",
        "ax[1].set_title('Distribution of Text Lengths (Words)')\n",
        "ax[1].set_xlabel('Number of Words')\n",
        "ax[1].axvline(x=CFG.max_token_len, color='red', linestyle='--', label=f'CLIP Max Tokens ({CFG.max_token_len})')\n",
        "ax[1].legend()\n",
        "plt.suptitle('Data Characteristics Analysis', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xk6EIx0ouyrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Displaying a Sample**"
      ],
      "metadata": {
        "id": "R81Il63lxINe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying a Sample\n",
        "print(\"\\n Displaying a Random Sample\")\n",
        "hate_sample = train_df[train_df['folder_name'] == 'Hate'].sample(1).iloc[0]\n",
        "nohate_sample = train_df[train_df['folder_name'] == 'No Hate'].sample(1).iloc[0]\n",
        "\n",
        "for sample in [hate_sample, nohate_sample]:\n",
        "    image_path = os.path.join(CFG.train_image_dir, sample['folder_name'], sample['index'])\n",
        "    try:\n",
        "        image = Image.open(image_path)\n",
        "        print(f\"\\nImage: {sample['index']}\")\n",
        "        print(f\"Folder Name: '{sample['folder_name']}' -> Encoded as: {sample['label_encoded']} (OFFICIAL)\")\n",
        "        print(f\"Text: '{sample['text']}'\")\n",
        "        plt.figure()\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"\\n Could not display sample. Image not found at: {image_path}\")\n"
      ],
      "metadata": {
        "id": "eER2uoG-uwyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the CLIP Processor**"
      ],
      "metadata": {
        "id": "hjyyNW5W2npp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = CLIPProcessor.from_pretrained(CFG.model_name)\n",
        "print(\"Processor loaded.\")"
      ],
      "metadata": {
        "id": "IwiZKEZ4u1zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset and DataLoaders**"
      ],
      "metadata": {
        "id": "kEAsCTdL2ccL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF3qNoUMFT58"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset Class\n",
        "class HateSpeechDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, processor, image_dir, is_test=False):\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.is_test = is_test\n",
        "\n",
        "        all_image_paths = glob.glob(os.path.join(image_dir, '**/*.*'), recursive=True)\n",
        "        self.image_path_map = {os.path.basename(p): p for p in all_image_paths}\n",
        "        print(f\"   - Dataset initialized with {len(self.df)} samples. Found {len(self.image_path_map)} images in {image_dir}.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        # The DataLoader needs to know the total size of the dataset.\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # This defines how to retrieve and process a single item by its index.\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Text Processing\n",
        "        text = str(row.get('text', '')) # Get text, or empty string if it's missing (for test set)\n",
        "        # The processor's text part (tokenizer) converts the string to numerical IDs.\n",
        "        tokenized = self.processor(\n",
        "            text=text,\n",
        "            truncation=True,\n",
        "            max_length=CFG.max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = tokenized['input_ids'].squeeze(0)\n",
        "        attention_mask = tokenized['attention_mask'].squeeze(0)\n",
        "\n",
        "        # Image Processing\n",
        "        image_name = row['index']\n",
        "        image_path = self.image_path_map.get(image_name)\n",
        "\n",
        "        if image_path:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        else:\n",
        "            print(f\"WARNING: Image '{image_name}' not found. Using a black placeholder image.\")\n",
        "            image = Image.new('RGB', (CFG.image_size, CFG.image_size), 'black')\n",
        "\n",
        "        # The processor's vision part handles resizing, cropping, and normalization.\n",
        "        processed_image = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = processed_image['pixel_values'].squeeze(0)\n",
        "\n",
        "        # --- Assemble the final output dictionary ---\n",
        "        item = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'pixel_values': pixel_values\n",
        "        }\n",
        "        if not self.is_test:\n",
        "            item['label'] = torch.tensor(row['label_encoded'], dtype=torch.long)\n",
        "\n",
        "        return item\n",
        "\n",
        "# DataLoaders\n",
        "print(\"\\n Creating DataLoaders\")\n",
        "test_df = pd.read_csv(CFG.test_csv_path)\n",
        "\n",
        "train_dataset = HateSpeechDataset(train_df, processor, CFG.train_image_dir)\n",
        "val_dataset = HateSpeechDataset(val_df, processor, CFG.val_image_dir)\n",
        "test_dataset = HateSpeechDataset(test_df, processor, CFG.test_image_dir, is_test=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n",
        "\n",
        "print(f\" DataLoaders created.\")\n",
        "print(f\"   - Training batches:   {len(train_loader)}\")\n",
        "print(f\"   - Validation batches: {len(val_loader)}\")\n",
        "print(f\"   - Test batches:       {len(test_loader)}\")\n",
        "\n",
        "# Clean up memory\n",
        "del val_labels_df, val_text_df\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Model Architecture**"
      ],
      "metadata": {
        "id": "ReXddEEwxL5U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fulxq9QsFc0u"
      },
      "outputs": [],
      "source": [
        "# Model Architecture\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Defining the Model Architecture\n",
        "class CLIPForHateSpeech(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the custom model. It uses a pre-trained CLIP model\n",
        "    as its 'backbone' and adds a custom classification 'head'.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(model_name)\n",
        "        projection_dim = self.clip.projection_dim\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2 * projection_dim, projection_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(projection_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        outputs = self.clip(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            pixel_values=pixel_values\n",
        "        )\n",
        "        image_features = outputs.image_embeds\n",
        "        text_features = outputs.text_embeds\n",
        "\n",
        "        combined_features = torch.cat((image_features, text_features), dim=1).\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Instantiate the Model**"
      ],
      "metadata": {
        "id": "P56UVID52-ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CLIPForHateSpeech(\n",
        "    model_name=CFG.model_name,\n",
        "    num_classes=CFG.num_classes\n",
        ").to(CFG.device)\n",
        "print(f\"Model instantiated and moved to {CFG.device}.\")\n",
        "print(f\" Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ],
      "metadata": {
        "id": "-_6mOG3128m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the Training Engine**"
      ],
      "metadata": {
        "id": "WeZmYe2l3DEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW([\n",
        "    {'params': model.clip.parameters(), 'lr': CFG.learning_rate_base},\n",
        "    {'params': model.classifier.parameters(), 'lr': CFG.learning_rate_head}\n",
        "])\n",
        "\n",
        "num_training_steps = CFG.epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=int(0.1 * num_training_steps),\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "print(\"Loss, Optimizer, and LR Scheduler are ready.\")"
      ],
      "metadata": {
        "id": "oSsV5gVD26fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Validation helper functions"
      ],
      "metadata": {
        "id": "oJhvcemz3Nqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Helper Functions\n",
        "def train_one_epoch(model, loader, optimizer, criterion, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(loader, desc=\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Perform a forward pass and calculate loss\n",
        "        outputs = model(input_ids, attention_mask, pixel_values)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation: calculate gradients and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad(): # Disable gradient calculations for validation\n",
        "        progress_bar = tqdm(loader, desc=\"Validating\")\n",
        "        for batch in progress_bar:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask, pixel_values)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    # Calculate our key performance metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    return avg_loss, accuracy, precision, recall, f1\n",
        "\n",
        "print(\"Training and validation helper functions are defined.\")"
      ],
      "metadata": {
        "id": "JVC0JdsW3LKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Main Training Loop**"
      ],
      "metadata": {
        "id": "JSXoGOPXxQE0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df6S4aHOn_6O"
      },
      "outputs": [],
      "source": [
        "# The Main Training Loop\n",
        "history = {\n",
        "    'train_loss': [], 'val_loss': [], 'val_accuracy': [],\n",
        "    'val_precision': [], 'val_recall': [], 'val_f1': []\n",
        "}\n",
        "best_val_f1 = 0.0\n",
        "model_path = os.path.join(CFG.output_dir, 'best_model_finetuned.pth')\n",
        "\n",
        "print(\"--- Starting Full Model Fine-Tuning ---\")\n",
        "\n",
        "for epoch in range(CFG.epochs):\n",
        "    print(f\"\\n===== Epoch {epoch + 1}/{CFG.epochs} =====\")\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, lr_scheduler, CFG.device)\n",
        "\n",
        "    # Validate the model\n",
        "    val_loss, val_acc, val_prec, val_rec, val_f1 = validate_one_epoch(model, val_loader, criterion, CFG.device)\n",
        "\n",
        "    # Store the results for this epoch\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_accuracy'].append(val_acc)\n",
        "    history['val_precision'].append(val_prec)\n",
        "    history['val_recall'].append(val_rec)\n",
        "    history['val_f1'].append(val_f1)\n",
        "\n",
        "    # Print the summary for the epoch\n",
        "    print(f\"Epoch {epoch + 1} Summary:\")\n",
        "    print(f\"  - Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  - Val Loss:   {val_loss:.4f}\")\n",
        "    print(\"  --- Validation Metrics ---\")\n",
        "    print(f\"  - Accuracy:  {val_acc:.4f}\")\n",
        "    print(f\"  - Precision: {val_prec:.4f}\")\n",
        "    print(f\"  - Recall:    {val_rec:.4f}\")\n",
        "    print(f\"  - F1-Score:  {val_f1:.4f}\")\n",
        "\n",
        "    # Save the model based on the best F1-score\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        print(f\"New best F1-score! Saving model to {model_path}\")\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    else:\n",
        "        print(\"F1-score did not improve.\")\n",
        "\n",
        "print(\"\\n Training Finished\")\n",
        "print(f\"Best validation F1-Score achieved: {best_val_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualizaing Training History**"
      ],
      "metadata": {
        "id": "XMaLHA8HxSsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the Training History\n",
        "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.plot(history['train_loss'], 'r-o', label='Train Loss')\n",
        "ax1.plot(history['val_loss'], 'orange', marker='o', label='Validation Loss')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend(loc='upper left')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(history['val_f1'], 'b-x', label='Validation F1-Score')\n",
        "ax2.set_ylabel('F1-Score')\n",
        "ax2.legend(loc='upper right')\n",
        "plt.title('Training and Validation History')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JCYBjubWvX0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exporting the test json**"
      ],
      "metadata": {
        "id": "pJLO6t-qxWSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Main Prediction Function\n",
        "def predict(model_path, test_loader, device):\n",
        "    model = CLIPForHateSpeech(\n",
        "        model_name=CFG.model_name,\n",
        "        num_classes=CFG.num_classes\n",
        "    ).to(device)\n",
        "\n",
        "    # Load the weights from the best saved model.\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_indices = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(test_loader, desc=\"Predicting on Test Set\")\n",
        "        for i, batch in enumerate(progress_bar):\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask, pixel_values)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "            start_idx = i * CFG.batch_size\n",
        "            end_idx = start_idx + len(preds)\n",
        "            batch_indices = test_df['index'][start_idx:end_idx].tolist()\n",
        "            all_indices.extend(batch_indices)\n",
        "\n",
        "    return all_indices, all_preds\n",
        "\n",
        "# Run the prediction process\n",
        "model_path = os.path.join(CFG.output_dir, 'best_model_finetuned.pth')\n",
        "indices, predictions = predict(model_path, test_loader, CFG.device)\n",
        "\n",
        "submission_path = os.path.join(CFG.output_dir, 'submission-taskA.json')\n",
        "\n",
        "with open(submission_path, 'w') as f:\n",
        "    for index, prediction in zip(indices, predictions):\n",
        "        # Create a dictionary for the current prediction\n",
        "        result = {\n",
        "            \"index\": index,\n",
        "            \"prediction\": int(prediction)\n",
        "        }\n",
        "        f.write(json.dumps(result) + '\\n')\n",
        "\n",
        "print(f\"\\n Submission file created successfully at: {submission_path}\")\n",
        "\n",
        "#Preview the first few lines of the created file\n",
        "print(\"\\n--- Submission File Preview ---\")\n",
        "!head -n 3 \"{submission_path}\"\n"
      ],
      "metadata": {
        "id": "sElqSchEP3Q6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}