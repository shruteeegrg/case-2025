{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount Google Drive**"
      ],
      "metadata": {
        "id": "100K_UB05x65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "XrnR2mqR5w7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Neccesary Packages**"
      ],
      "metadata": {
        "id": "kK4-Wchy5vRW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2E7WYQNW-7C"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets scikit-learn seaborn\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import CLIPProcessor, CLIPTokenizer, CLIPModel, get_scheduler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "import glob\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function for Configuring Files and Hyperparamters**"
      ],
      "metadata": {
        "id": "ezIETqlt7yHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG_C:\n",
        "    \"\"\"\n",
        "    Configuration Class for Subtask C: Stance Detection.\n",
        "    \"\"\"\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/case-2025\"\n",
        "\n",
        "    # File & Directory Paths\n",
        "    train_dir = os.path.join(DRIVE_PATH, \"SubTaskC/Train\")\n",
        "    train_text_path = os.path.join(DRIVE_PATH, \"SubTaskC/Train/STask_C_train.csv\")\n",
        "    val_image_dir = os.path.join(DRIVE_PATH, \"SubTaskC/Eval/STask_C_val_img\")\n",
        "    val_labels_path = os.path.join(DRIVE_PATH, \"SubTaskC/Eval/STask-C(index,label)val.csv\")\n",
        "    val_text_path = os.path.join(DRIVE_PATH, \"SubTaskC/Eval/STask-C(index,text)val.csv\")\n",
        "    test_image_dir = os.path.join(DRIVE_PATH, \"SubTaskC/Test/STask_C_test_img\")\n",
        "    test_csv_path = os.path.join(DRIVE_PATH, \"SubTaskC/Test/STask-C(index,text)test.csv\")\n",
        "    output_dir = os.path.join(DRIVE_PATH, \"output_subtask_c\")\n",
        "\n",
        "    # Model & Training Parameters\n",
        "    model_name = 'openai/clip-vit-large-patch14'\n",
        "    image_size = 224\n",
        "    max_token_len = 77\n",
        "    learning_rate_base = 1e-6\n",
        "    learning_rate_head = 1e-5\n",
        "    batch_size = 16\n",
        "    epochs = 10\n",
        "    num_workers = 2\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "os.makedirs(CFG_C.output_dir, exist_ok=True)\n",
        "print(f\"Subtask C Configuration defined. Output will be saved to: {CFG_C.output_dir}\")\n",
        "\n",
        "# Data Loading & Reconnaissance\n",
        "print(\"\\nLoading Subtask C data...\")\n",
        "try:\n",
        "    #Training Data Loading\n",
        "    train_image_paths = glob.glob(os.path.join(CFG_C.train_dir, '**/*.png'), recursive=True)\n",
        "    train_data = []\n",
        "    class_folders = ['Neutral', 'Support', 'Oppose']\n",
        "    for path in train_image_paths:\n",
        "        label = os.path.basename(os.path.dirname(path))\n",
        "        if label in class_folders:\n",
        "            train_data.append({'index': os.path.basename(path), 'label_text': label})\n",
        "\n",
        "    ground_truth_labels_df = pd.DataFrame(train_data)\n",
        "    text_data_df = pd.read_csv(CFG_C.train_text_path, usecols=['index', 'text'])\n",
        "    train_df_c = pd.merge(ground_truth_labels_df, text_data_df, on=\"index\")\n",
        "\n",
        "    #Validation Data Loading\n",
        "    val_labels_df = pd.read_csv(CFG_C.val_labels_path)\n",
        "    val_text_df = pd.read_csv(CFG_C.val_text_path)\n",
        "    val_df_c = pd.merge(val_text_df, val_labels_df, on=\"index\")\n",
        "\n",
        "    print(\"Data loaded successfully.\")\n",
        "    print(f\"Training samples:   {len(train_df_c)}\")\n",
        "    print(f\"Validation samples: {len(val_df_c)}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\nFATAL ERROR: A data file was not found. Please double-check your paths in CFG_C.\")\n",
        "    print(f\"Details: {e}\")\n",
        "    sys.exit()"
      ],
      "metadata": {
        "id": "wjLCuXPjXHhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization: Label Distribution"
      ],
      "metadata": {
        "id": "AzpuJDyV8G1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.figure(figsize=(10, 6))\n",
        "class_order = ['Neutral', 'Support', 'Oppose']\n",
        "sns.countplot(x='label_text', data=train_df_c, palette='crest', order=class_order)\n",
        "plt.title('Subtask C: Training Set Label Distribution')\n",
        "plt.xlabel('Stance Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hu7A-f2e8E0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PREPARATION**"
      ],
      "metadata": {
        "id": "V-JbvQA28Rmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preparing Subtask C data: Enforcing official label encoding.\")\n",
        "official_target_map_c = {\n",
        "    'Neutral': 0,\n",
        "    'Support': 1,\n",
        "    'Oppose': 2\n",
        "}\n",
        "print(f\"   - Official Target Map: {official_target_map_c}\")\n",
        "\n",
        "# Create the 'label_encoded' column for the training set from the 'label_text' column.\n",
        "train_df_c['label_encoded'] = train_df_c['label_text'].map(official_target_map_c)\n",
        "\n",
        "val_df_c.rename(columns={'label': 'label_encoded'}, inplace=True)\n",
        "val_df_c['label_encoded'] = val_df_c['label_encoded'].astype(int)\n",
        "\n",
        "if train_df_c['label_encoded'].isnull().any() or val_df_c['label_encoded'].isnull().any():\n",
        "    print(\" WARNING: Null values found after processing. Please investigate.\")\n",
        "else:\n",
        "    print(\"Label encoding complete and columns aligned.\")\n",
        "\n",
        "CFG_C.num_classes = len(official_target_map_c)\n",
        "print(f\"   - Number of classes for Subtask C: {CFG_C.num_classes}\")"
      ],
      "metadata": {
        "id": "dHoESPCEXM5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BUILDING THE DATA PIPELINE**"
      ],
      "metadata": {
        "id": "bFTn8qS_8dZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Official CLIP Processor\n",
        "processor = CLIPProcessor.from_pretrained(CFG_C.model_name)\n",
        "\n",
        "# Define the  Dataset Class\n",
        "class StanceDataset(Dataset):\n",
        "    def __init__(self, df, processor, image_dir, image_size, max_token_len, is_test=False):\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.image_dir = image_dir\n",
        "        self.image_size = image_size\n",
        "        self.max_token_len = max_token_len\n",
        "        self.is_test = is_test\n",
        "        self.image_path_map = {os.path.basename(p): p for p in glob.glob(os.path.join(image_dir, '**/*.*'), recursive=True)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = str(row.get('text', ''))\n",
        "\n",
        "        tokenized = self.processor(text=text, truncation=True, max_length=self.max_token_len, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        image_name = row['index']\n",
        "        image_path = self.image_path_map.get(image_name)\n",
        "        if image_path:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        else:\n",
        "            image = Image.new('RGB', (self.image_size, self.image_size), 'black')\n",
        "        processed_image = self.processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "        # Assemble the item\n",
        "        item = {\n",
        "            'input_ids': tokenized['input_ids'].squeeze(0),\n",
        "            'attention_mask': tokenized['attention_mask'].squeeze(0),\n",
        "            'pixel_values': processed_image['pixel_values'].squeeze(0)\n",
        "        }\n",
        "        if not self.is_test:\n",
        "            item['label'] = torch.tensor(row['label_encoded'], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "# DataLoaders for Subtask C\n",
        "test_df_c = pd.read_csv(CFG_C.test_csv_path)\n",
        "\n",
        "train_dataset_c = StanceDataset(train_df_c, processor, CFG_C.train_dir, CFG_C.image_size, CFG_C.max_token_len)\n",
        "val_dataset_c = StanceDataset(val_df_c, processor, CFG_C.val_image_dir, CFG_C.image_size, CFG_C.max_token_len)\n",
        "test_dataset_c = StanceDataset(test_df_c, processor, CFG_C.test_image_dir, CFG_C.image_size, CFG_C.max_token_len, is_test=True)\n",
        "\n",
        "train_loader_c = DataLoader(train_dataset_c, batch_size=CFG_C.batch_size, shuffle=True, num_workers=CFG_C.num_workers)\n",
        "val_loader_c = DataLoader(val_dataset_c, batch_size=CFG_C.batch_size, shuffle=False, num_workers=CFG_C.num_workers)\n",
        "test_loader_c = DataLoader(test_dataset_c, batch_size=CFG_C.batch_size, shuffle=False, num_workers=CFG_C.num_workers)\n",
        "\n",
        "print(f\"DataLoaders created.\")\n",
        "print(f\"Training batches:   {len(train_loader_c)}\")\n",
        "print(f\"Validation batches: {len(val_loader_c)}\")\n",
        "print(f\"Test batches:       {len(test_loader_c)}\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "RqAAnkqjXNtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Model Architecture**"
      ],
      "metadata": {
        "id": "IWmRaqab9H8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the Model Architecture\n",
        "class StanceClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(model_name)\n",
        "        projection_dim = self.clip.projection_dim\n",
        "\n",
        "        # A deeper, 3-layer classifier for more capacity.\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2 * projection_dim, projection_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4), # Slightly increased dropout for the larger head\n",
        "            nn.Linear(projection_dim * 2, projection_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(projection_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        outputs = self.clip(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            pixel_values=pixel_values\n",
        "        )\n",
        "        image_features = outputs.image_embeds\n",
        "        text_features = outputs.text_embeds\n",
        "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "sFgkCnj4XUmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the Model"
      ],
      "metadata": {
        "id": "OOL-65R-9UZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_c = StanceClassifier(\n",
        "    model_name=CFG_C.model_name,\n",
        "    num_classes=CFG_C.num_classes\n",
        ").to(CFG_C.device)\n",
        "print(f\"Model instantiated.\")"
      ],
      "metadata": {
        "id": "8ueQQyl89VUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the Training Engine with COSINE Scheduler**"
      ],
      "metadata": {
        "id": "0nCiSdkE9jve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW([\n",
        "    {'params': model_c.clip.parameters(), 'lr': CFG_C.learning_rate_base},\n",
        "    {'params': model_c.classifier.parameters(), 'lr': CFG_C.learning_rate_head}\n",
        "])\n",
        "num_training_steps = CFG_C.epochs * len(train_loader_c)\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"cosine\", # Using the more advanced cosine scheduler\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=int(0.1 * num_training_steps),\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "print(\"Optimizer and COSINE LR Scheduler are ready.\")"
      ],
      "metadata": {
        "id": "729sSL1R9SKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "9ikE7SO79tAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n",
        "    for batch in progress_bar:\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids, attention_mask, pixel_values)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(loader, desc=\"Validating\", leave=False)\n",
        "        for batch in progress_bar:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask, pixel_values)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    return avg_loss, accuracy, precision, recall, f1\n",
        "\n",
        "print(\"Helper functions are ready.\")"
      ],
      "metadata": {
        "id": "iMYU37kd9nI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Main Training Loop**"
      ],
      "metadata": {
        "id": "y6OPHVNt9v7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_c = {\n",
        "    'train_loss': [], 'val_loss': [], 'val_accuracy': [],\n",
        "    'val_precision': [], 'val_recall': [], 'val_f1': []\n",
        "}\n",
        "best_val_f1 = 0.0\n",
        "model_path_c = os.path.join(CFG_C.output_dir, 'best_model_subtask_c_advanced.pth')\n",
        "\n",
        "print(\"\\n Starting Training for Subtask C: \")\n",
        "for epoch in range(CFG_C.epochs):\n",
        "    print(f\"\\n===== Epoch {epoch + 1}/{CFG_C.epochs} =====\")\n",
        "    train_loss = train_one_epoch(model_c, train_loader_c, optimizer, criterion, lr_scheduler, CFG_C.device)\n",
        "    val_loss, val_acc, val_prec, val_rec, val_f1 = validate_one_epoch(model_c, val_loader_c, criterion, CFG_C.device)\n",
        "    history_c['train_loss'].append(train_loss); history_c['val_loss'].append(val_loss)\n",
        "    history_c['val_accuracy'].append(val_acc); history_c['val_precision'].append(val_prec)\n",
        "    history_c['val_recall'].append(val_rec); history_c['val_f1'].append(val_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} Summary:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"Accuracy: {val_acc:.4f}, F1-Score (Macro): {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        print(f\"New best F1-score! Saving model to {model_path_c}\")\n",
        "        torch.save(model_c.state_dict(), model_path_c)\n",
        "    else:\n",
        "        print(\"F1-score did not improve.\")\n",
        "\n",
        "print(\"\\n Training Finished \")\n",
        "print(f\"Best validation F1-Score for Subtask C achieved: {best_val_f1:.4f}\")"
      ],
      "metadata": {
        "id": "EZiQjxKa9qBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prediction on Test Dataset**"
      ],
      "metadata": {
        "id": "RjLZkIW099Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Prediction Function\n",
        "def predict_subtask_c(model_path, test_loader, device):\n",
        "    print(\"--> Instantiating model architecture for prediction...\")\n",
        "    model = StanceClassifier(\n",
        "        model_name=CFG_C.model_name,\n",
        "        num_classes=CFG_C.num_classes\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"--> Loading best model weights onto device: {device}\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(test_loader, desc=\"Predicting on Test Set\")\n",
        "        for batch in progress_bar:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask, pixel_values)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    return all_preds\n",
        "\n",
        "# Run the Prediction and Create the Submission File\n",
        "model_path = os.path.join(CFG_C.output_dir, 'best_model_subtask_c_advanced.pth')\n",
        "predictions = predict_subtask_c(model_path, test_loader_c, CFG_C.device)\n",
        "\n",
        "indices = test_df_c['index'].tolist()\n",
        "\n",
        "print(\"\\n Creating JSON Lines submission file: \")\n",
        "submission_path = os.path.join(CFG_C.output_dir, 'submission.json')\n",
        "\n",
        "with open(submission_path, 'w') as f:\n",
        "    for index, prediction in zip(indices, predictions):\n",
        "        result = {\n",
        "            \"index\": index,\n",
        "            \"prediction\": int(prediction)\n",
        "        }\n",
        "        f.write(json.dumps(result) + '\\n')\n",
        "\n",
        "print(f\"\\nSubmission file for Subtask C created successfully at: {submission_path}\")"
      ],
      "metadata": {
        "id": "fkKXzSDjXVSU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}