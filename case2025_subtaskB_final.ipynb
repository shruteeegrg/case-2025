{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount Google Drive**\n",
        "\n"
      ],
      "metadata": {
        "id": "ukCwp9RBs6yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vyc9Mr1Js55D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Neccesary Packages**"
      ],
      "metadata": {
        "id": "JJ9JDq5Qtds-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets scikit-learn seaborn\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import CLIPProcessor, CLIPTokenizer, CLIPModel, get_scheduler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "import glob\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "T0GCQnL-Y2ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function for Configuring Files and Hyperparamters**"
      ],
      "metadata": {
        "id": "p31dvng-uFCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "class CFG_B:\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/case-2025\"\n",
        "\n",
        "    train_dir = os.path.join(DRIVE_PATH, \"SubTaskB/Train\")\n",
        "    train_text_path = os.path.join(DRIVE_PATH, \"SubTaskB/Train/STask_B_train.csv\")\n",
        "\n",
        "    val_image_dir = os.path.join(DRIVE_PATH, \"SubTaskB/Eval/STask_B_val_img\")\n",
        "    val_labels_path = os.path.join(DRIVE_PATH, \"SubTaskB/Eval/STask-B(index,label)val.csv\")\n",
        "    val_text_path = os.path.join(DRIVE_PATH, \"SubTaskB/Eval/STask-B(index,text)val.csv\")\n",
        "\n",
        "    test_image_dir = os.path.join(DRIVE_PATH, \"SubTaskB/Test/STask_B_test_img\")\n",
        "    test_csv_path = os.path.join(DRIVE_PATH, \"SubTaskB/Test/STask-B(index,text)test.csv\")\n",
        "\n",
        "    output_dir = os.path.join(DRIVE_PATH, \"output_subtask_b_resampled\")\n",
        "    model_name = 'openai/clip-vit-large-patch14'\n",
        "    image_size = 224\n",
        "    max_token_len = 77\n",
        "    learning_rate_base = 1e-6\n",
        "    learning_rate_head = 1e-5\n",
        "    batch_size = 16\n",
        "    epochs = 8\n",
        "    num_workers = 2\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "os.makedirs(CFG_B.output_dir, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    train_image_paths = glob.glob(os.path.join(CFG_B.train_dir, '**/*.png'), recursive=True)\n",
        "    train_data = []\n",
        "    class_folders = ['Undirected', 'Individual', 'Community', 'Organization']\n",
        "    for path in train_image_paths:\n",
        "        label = os.path.basename(os.path.dirname(path))\n",
        "        if label in class_folders:\n",
        "            train_data.append({'index': os.path.basename(path), 'label_text': label})\n",
        "    ground_truth_labels_df = pd.DataFrame(train_data)\n",
        "    text_data_df = pd.read_csv(CFG_B.train_text_path, usecols=['index', 'text'])\n",
        "    train_df_b = pd.merge(ground_truth_labels_df, text_data_df, on=\"index\")\n",
        "    val_labels_df = pd.read_csv(CFG_B.val_labels_path)\n",
        "    val_text_df = pd.read_csv(CFG_B.val_text_path)\n",
        "    val_df_b = pd.merge(val_text_df, val_labels_df, on=\"index\")\n",
        "    print(\"Data loaded successfully.\")\n",
        "\n",
        "    # Enforcing Label Encoding\n",
        "    official_target_map = {'Undirected': 0, 'Individual': 1, 'Community': 2, 'Organization': 3}\n",
        "    train_df_b['label_encoded'] = train_df_b['label_text'].map(official_target_map)\n",
        "    val_df_b.rename(columns={'label': 'label_encoded'}, inplace=True)\n",
        "    val_df_b['label_encoded'] = val_df_b['label_encoded'].astype(int)\n",
        "\n",
        "    CFG_B.num_classes = len(official_target_map)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n ERROR during data processing: {e}\")\n",
        "    sys.exit()"
      ],
      "metadata": {
        "id": "nIgtkg7wXc-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OVER-SAMPLING THE TRAINING DATA"
      ],
      "metadata": {
        "id": "8BXEucEkv043"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the majority class\n",
        "majority_class_name = train_df_b['label_text'].value_counts().idxmax()\n",
        "majority_class_size = len(train_df_b[train_df_b['label_text'] == majority_class_name])\n",
        "print(f\"Majority class '{majority_class_name}' has {majority_class_size} samples.\")\n",
        "\n",
        "\n",
        "resampled_dfs = []\n",
        "# Loop through each unique class\n",
        "for class_name in train_df_b['label_text'].unique():\n",
        "    class_df = train_df_b[train_df_b['label_text'] == class_name]\n",
        "    if class_name == majority_class_name:\n",
        "        # If it's the majority class, just add it as is\n",
        "        resampled_dfs.append(class_df)\n",
        "    else:\n",
        "        # If it's a minority class, over-sample it with replacement\n",
        "        resampled_class_df = resample(class_df,\n",
        "                                      replace=True, # Sample with replacement\n",
        "                                      n_samples=majority_class_size, # Match majority class size\n",
        "                                      random_state=42) # For reproducibility\n",
        "        resampled_dfs.append(resampled_class_df)\n",
        "\n",
        "# Concatenate the dataframes to create the new, balanced training set\n",
        "train_df_b_resampled = pd.concat(resampled_dfs)\n",
        "print(f\"Over-sampling complete. New training set size: {len(train_df_b_resampled)}\")"
      ],
      "metadata": {
        "id": "gRIhkyjev0hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Visualization**"
      ],
      "metadata": {
        "id": "hUZf9KnTucf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "print(\"\\n--> Visualizing data characteristics...\")\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "# Original Distribution\n",
        "sns.countplot(x='label_text', data=train_df_b, ax=ax1, palette='viridis', order=class_folders)\n",
        "ax1.set_title('Original Training Set Distribution')\n",
        "ax1.set_xlabel('Target Class'); ax1.set_ylabel('Count'); ax1.tick_params(axis='x', rotation=15)\n",
        "# Resampled Distribution\n",
        "sns.countplot(x='label_text', data=train_df_b_resampled, ax=ax2, palette='magma', order=class_folders)\n",
        "ax2.set_title('Resampled Training Set Distribution')\n",
        "ax2.set_xlabel('Target Class'); ax2.set_ylabel('Count'); ax2.tick_params(axis='x', rotation=15)\n",
        "plt.suptitle('Data Distribution Before and After Over-sampling', fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g_YaSHSbuaaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset and DataLoaders**"
      ],
      "metadata": {
        "id": "_3rhl1GH0wRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the CLIP Processor\n",
        "processor = CLIPProcessor.from_pretrained(CFG_B.model_name)\n",
        "\n",
        "# Define the Self-Contained Dataset Class\n",
        "class HateSpeechDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for Subtask B.\n",
        "    This class is self-contained and receives all its dependencies directly.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, processor, image_dir, image_size, max_token_len, is_test=False):\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.image_dir = image_dir\n",
        "        self.image_size = image_size\n",
        "        self.max_token_len = max_token_len\n",
        "        self.is_test = is_test\n",
        "\n",
        "        self.image_path_map = {os.path.basename(p): p for p in glob.glob(os.path.join(image_dir, '**/*.*'), recursive=True)}\n",
        "        print(f\"   - Dataset initialized with {len(self.df)} samples. Found {len(self.image_path_map)} images.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = str(row.get('text', ''))\n",
        "        tokenized = self.processor(\n",
        "            text=text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = tokenized['input_ids'].squeeze(0)\n",
        "        attention_mask = tokenized['attention_mask'].squeeze(0)\n",
        "\n",
        "        image_name = row['index']\n",
        "        image_path = self.image_path_map.get(image_name)\n",
        "\n",
        "        if image_path:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        else:\n",
        "            image = Image.new('RGB', (self.image_size, self.image_size), 'black')\n",
        "\n",
        "        processed_image = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = processed_image['pixel_values'].squeeze(0)\n",
        "\n",
        "        item = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'pixel_values': pixel_values\n",
        "        }\n",
        "        if not self.is_test:\n",
        "            item['label'] = torch.tensor(row['label_encoded'], dtype=torch.long)\n",
        "\n",
        "        return item\n",
        "\n",
        "# DataLoaders for Subtask B\n",
        "test_df_b = pd.read_csv(CFG_B.test_csv_path)\n",
        "train_dataset_b = HateSpeechDataset(train_df_b_resampled, processor, CFG_B.train_dir, CFG_B.image_size, CFG_B.max_token_len)\n",
        "val_dataset_b = HateSpeechDataset(val_df_b, processor, CFG_B.val_image_dir, CFG_B.image_size, CFG_B.max_token_len)\n",
        "test_dataset_b = HateSpeechDataset(test_df_b, processor, CFG_B.test_image_dir, CFG_B.image_size, CFG_B.max_token_len, is_test=True)\n",
        "\n",
        "train_loader_b = DataLoader(train_dataset_b, batch_size=CFG_B.batch_size, shuffle=True, num_workers=CFG_B.num_workers)\n",
        "val_loader_b = DataLoader(val_dataset_b, batch_size=CFG_B.batch_size, shuffle=False, num_workers=CFG_B.num_workers)\n",
        "test_loader_b = DataLoader(test_dataset_b, batch_size=CFG_B.batch_size, shuffle=False, num_workers=CFG_B.num_workers)\n",
        "\n",
        "print(f\"DataLoaders created. Training loader is now using the resampled, balanced data.\")\n",
        "print(f\"Training batches:   {len(train_loader_b)}\")\n",
        "print(f\"Validation batches: {len(val_loader_b)}\")\n",
        "print(f\"Test batches:       {len(test_loader_b)}\")\n",
        "\n",
        "# Clean up to free memory\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "jTTyOdp6rDbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the Model Architecture**"
      ],
      "metadata": {
        "id": "pxuL8UqMzd6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPForHateSpeech(nn.Module):\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(model_name)\n",
        "        projection_dim = self.clip.projection_dim\n",
        "        # This is our original, more stable classifier architecture.\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2 * projection_dim, projection_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(projection_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        outputs = self.clip(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            pixel_values=pixel_values\n",
        "        )\n",
        "        image_features = outputs.image_embeds\n",
        "        text_features = outputs.text_embeds\n",
        "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "G0sY_AI2sb4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instantiate the Model"
      ],
      "metadata": {
        "id": "gDgsk-hI0hcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_b = CLIPForHateSpeech(\n",
        "    model_name=CFG_B.model_name,\n",
        "    num_classes=CFG_B.num_classes\n",
        ").to(CFG_B.device)\n",
        "print(f\"Model instantiated with {CFG_B.num_classes} output classes.\")"
      ],
      "metadata": {
        "id": "mL7OAzrw0fZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining the Training Engine with STANDARD Loss"
      ],
      "metadata": {
        "id": "IbVEwZKp0tQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = AdamW([\n",
        "    {'params': model_b.clip.parameters(), 'lr': CFG_B.learning_rate_base},\n",
        "    {'params': model_b.classifier.parameters(), 'lr': CFG_B.learning_rate_head}\n",
        "])\n",
        "num_training_steps = CFG_B.epochs * len(train_loader_b)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=int(0.1 * num_training_steps),\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "print(\"Optimizer and LR Scheduler are ready.\")"
      ],
      "metadata": {
        "id": "G7gIrZhX0dRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "PtK127Pq0pSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(loader, desc=\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids, attention_mask, pixel_values)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(loader, desc=\"Validating\")\n",
        "        for batch in progress_bar:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask, pixel_values)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    return avg_loss, accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "zIXdYwSy0oQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Training Loop**"
      ],
      "metadata": {
        "id": "6j-SA0Xw02Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_b = {\n",
        "    'train_loss': [], 'val_loss': [], 'val_accuracy': [],\n",
        "    'val_precision': [], 'val_recall': [], 'val_f1': []\n",
        "}\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "model_path_b = os.path.join(CFG_B.output_dir, 'best_model_subtask_b.pth')\n",
        "\n",
        "print(\"--- Starting Subtask B Fine-Tuning ---\")\n",
        "\n",
        "for epoch in range(CFG_B.epochs):\n",
        "    print(f\"\\n===== Epoch {epoch + 1}/{CFG_B.epochs} =====\")\n",
        "\n",
        "    train_loss = train_one_epoch(model_b, train_loader_b, optimizer, criterion, lr_scheduler, CFG_B.device)\n",
        "    val_loss, val_acc, val_prec, val_rec, val_f1 = validate_one_epoch(model_b, val_loader_b, criterion, CFG_B.device)\n",
        "\n",
        "    history_b['train_loss'].append(train_loss)\n",
        "    history_b['val_loss'].append(val_loss)\n",
        "    history_b['val_accuracy'].append(val_acc)\n",
        "    history_b['val_precision'].append(val_prec)\n",
        "    history_b['val_recall'].append(val_rec)\n",
        "    history_b['val_f1'].append(val_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} Summary:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val Loss:   {val_loss:.4f}\")\n",
        "    print(\" - Validation Metrics -\")\n",
        "    print(f\"Accuracy:  {val_acc:.4f}\")\n",
        "    print(f\"Precision (Macro): {val_prec:.4f}\")\n",
        "    print(f\"Recall (Macro):    {val_rec:.4f}\")\n",
        "    print(f\"F1-Score (Macro):  {val_f1:.4f}\")\n",
        "\n",
        "    # Save the model based on the best F1-score\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        print(f\"New best F1-score! Saving model to {model_path_b}\")\n",
        "        torch.save(model_b.state_dict(), model_path_b)\n",
        "    else:\n",
        "        print(\"  - F1-score did not improve.\")\n",
        "\n",
        "print(\"\\n Training Finished\")\n",
        "print(f\"Best validation F1-Score for Subtask B achieved: {best_val_f1:.4f}\")"
      ],
      "metadata": {
        "id": "d_CXzwwcux2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the Training History"
      ],
      "metadata": {
        "id": "0P_aCDZq05_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.plot(history_b['train_loss'], 'r-o', label='Train Loss')\n",
        "ax1.plot(history_b['val_loss'], 'orange', marker='o', label='Validation Loss')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend(loc='upper left')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(history_b['val_f1'], 'b-x', label='Validation F1-Score (Macro)')\n",
        "ax2.set_ylabel('F1-Score (Macro)')\n",
        "ax2.legend(loc='upper right')\n",
        "plt.title('Subtask B: Training and Validation History')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SF14vhPp05u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Prediction on Test Set **"
      ],
      "metadata": {
        "id": "r0N82k5J1HkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class CLIPForHateSpeech(nn.Module):\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(model_name)\n",
        "        projection_dim = self.clip.projection_dim\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2 * projection_dim, projection_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(projection_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        outputs = self.clip(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            pixel_values=pixel_values\n",
        "        )\n",
        "        image_features = outputs.image_embeds\n",
        "        text_features = outputs.text_embeds\n",
        "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits\n",
        "\n",
        "# Define the Prediction Function\n",
        "def predict_subtask_b(model_path, test_loader, device):\n",
        "    print(\"--> Instantiating model architecture for prediction...\")\n",
        "    model = CLIPForHateSpeech(\n",
        "        model_name=CFG_B.model_name,\n",
        "        num_classes=CFG_B.num_classes\n",
        "    ).to(device)\n",
        "\n",
        "    # Load the saved weights from your best model\n",
        "    print(f\"--> Loading best model weights from: {model_path}\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(test_loader, desc=\"Predicting on Test Set\")\n",
        "        for batch in progress_bar:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask, pixel_values)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    return all_preds\n",
        "\n",
        "# Run the Prediction and Create the Submission File\n",
        "model_path = os.path.join(CFG_B.output_dir, 'best_model_subtask_b.pth')\n",
        "predictions = predict_subtask_b(model_path, test_loader_b, CFG_B.device)\n",
        "\n",
        "indices = test_df_b['index'].tolist()\n",
        "\n",
        "submission_path = os.path.join(CFG_B.output_dir, 'submission.json')\n",
        "\n",
        "with open(submission_path, 'w') as f:\n",
        "    for index, prediction in zip(indices, predictions):\n",
        "\n",
        "        result = {\n",
        "            \"index\": index,\n",
        "            \"prediction\": int(prediction)\n",
        "        }\n",
        "        f.write(json.dumps(result) + '\\n')\n",
        "\n",
        "print(f\"\\n Submission file for Subtask B created successfully at: {submission_path}\")\n",
        "\n",
        "# The first few lines of the created file\n",
        "print(\"\\n Submission File Preview: \")\n",
        "!head -n 5 \"{submission_path}\""
      ],
      "metadata": {
        "id": "nrrYqtAjwejy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}